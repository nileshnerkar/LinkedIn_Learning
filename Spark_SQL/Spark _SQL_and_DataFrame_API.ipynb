{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing SparkSession library\n",
    "**Once we import SparkSession we dotn need to import Spark Context and SQL Context separately**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-OV6LJTB:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x21a1b4ac898>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting up a path variable for CSV location**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/C:/Users/niles/Downloads/IS_Courses/Spark LinkedIn/Ex_Files_Spark_SQL_DataFrames/Exercise Files/Data/location_temp.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.read.format(\"csv\").option(\"header\",\"true\").load(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+------------+\n",
      "|         event_date|location_id|temp_celcius|\n",
      "+-------------------+-----------+------------+\n",
      "|03/04/2019 19:48:06|       loc0|          29|\n",
      "|03/04/2019 19:53:06|       loc0|          27|\n",
      "|03/04/2019 19:58:06|       loc0|          28|\n",
      "|03/04/2019 20:03:06|       loc0|          30|\n",
      "|03/04/2019 20:08:06|       loc0|          27|\n",
      "|03/04/2019 20:13:06|       loc0|          27|\n",
      "|03/04/2019 20:18:06|       loc0|          27|\n",
      "|03/04/2019 20:23:06|       loc0|          29|\n",
      "|03/04/2019 20:28:06|       loc0|          32|\n",
      "|03/04/2019 20:33:06|       loc0|          35|\n",
      "|03/04/2019 20:38:06|       loc0|          32|\n",
      "|03/04/2019 20:43:06|       loc0|          28|\n",
      "|03/04/2019 20:48:06|       loc0|          28|\n",
      "|03/04/2019 20:53:06|       loc0|          32|\n",
      "|03/04/2019 20:58:06|       loc0|          34|\n",
      "|03/04/2019 21:03:06|       loc0|          33|\n",
      "|03/04/2019 21:08:06|       loc0|          27|\n",
      "|03/04/2019 21:13:06|       loc0|          28|\n",
      "|03/04/2019 21:18:06|       loc0|          33|\n",
      "|03/04/2019 21:23:06|       loc0|          28|\n",
      "+-------------------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FILTER Data with DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+------------+\n",
      "|         event_date|location_id|temp_celcius|\n",
      "+-------------------+-----------+------------+\n",
      "|03/04/2019 19:48:06|       loc0|          29|\n",
      "|03/04/2019 19:53:06|       loc0|          27|\n",
      "|03/04/2019 19:58:06|       loc0|          28|\n",
      "|03/04/2019 20:03:06|       loc0|          30|\n",
      "|03/04/2019 20:08:06|       loc0|          27|\n",
      "|03/04/2019 20:13:06|       loc0|          27|\n",
      "|03/04/2019 20:18:06|       loc0|          27|\n",
      "|03/04/2019 20:23:06|       loc0|          29|\n",
      "|03/04/2019 20:28:06|       loc0|          32|\n",
      "|03/04/2019 20:33:06|       loc0|          35|\n",
      "|03/04/2019 20:38:06|       loc0|          32|\n",
      "|03/04/2019 20:43:06|       loc0|          28|\n",
      "|03/04/2019 20:48:06|       loc0|          28|\n",
      "|03/04/2019 20:53:06|       loc0|          32|\n",
      "|03/04/2019 20:58:06|       loc0|          34|\n",
      "|03/04/2019 21:03:06|       loc0|          33|\n",
      "|03/04/2019 21:08:06|       loc0|          27|\n",
      "|03/04/2019 21:13:06|       loc0|          28|\n",
      "|03/04/2019 21:18:06|       loc0|          33|\n",
      "|03/04/2019 21:23:06|       loc0|          28|\n",
      "+-------------------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.filter(df1[\"location_id\"]==\"loc0\").show()  #df1= df1[df1['location'] =='loc0'] in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.filter(df1[\"location_id\"]==\"loc0\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.filter(df1[\"location_id\"]==\"loc1\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.filter(df1[\"location_id\"]==\"loc0\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AGGREGATION of Data with DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|location_id|count|\n",
      "+-----------+-----+\n",
      "|     loc196| 1000|\n",
      "|     loc226| 1000|\n",
      "|     loc463| 1000|\n",
      "|     loc150| 1000|\n",
      "|     loc292| 1000|\n",
      "|     loc311| 1000|\n",
      "|      loc22| 1000|\n",
      "|     loc351| 1000|\n",
      "|     loc370| 1000|\n",
      "|     loc419| 1000|\n",
      "|      loc31| 1000|\n",
      "|     loc305| 1000|\n",
      "|      loc82| 1000|\n",
      "|      loc90| 1000|\n",
      "|     loc118| 1000|\n",
      "|     loc195| 1000|\n",
      "|     loc208| 1000|\n",
      "|      loc39| 1000|\n",
      "|      loc75| 1000|\n",
      "|     loc228| 1000|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy(\"location_id\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+------------+\n",
      "|         event_date|location_id|temp_celcius|\n",
      "+-------------------+-----------+------------+\n",
      "|03/04/2019 21:23:06|       loc0|          28|\n",
      "|03/04/2019 20:43:06|       loc0|          28|\n",
      "|03/04/2019 21:18:06|       loc0|          33|\n",
      "|03/04/2019 20:18:06|       loc0|          27|\n",
      "|03/04/2019 20:38:06|       loc0|          32|\n",
      "|03/04/2019 20:58:06|       loc0|          34|\n",
      "|03/04/2019 21:13:06|       loc0|          28|\n",
      "|03/04/2019 19:58:06|       loc0|          28|\n",
      "|03/04/2019 20:13:06|       loc0|          27|\n",
      "|03/04/2019 20:28:06|       loc0|          32|\n",
      "|03/04/2019 20:33:06|       loc0|          35|\n",
      "|03/04/2019 20:48:06|       loc0|          28|\n",
      "|03/04/2019 20:53:06|       loc0|          32|\n",
      "|03/04/2019 21:03:06|       loc0|          33|\n",
      "|03/04/2019 21:08:06|       loc0|          27|\n",
      "|03/04/2019 19:48:06|       loc0|          29|\n",
      "|03/04/2019 19:53:06|       loc0|          27|\n",
      "|03/04/2019 20:03:06|       loc0|          30|\n",
      "|03/04/2019 20:08:06|       loc0|          27|\n",
      "|03/04/2019 20:23:06|       loc0|          29|\n",
      "+-------------------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.orderBy(\"location_id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MEAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n",
      "|location_id|avg(temp_celcius)|\n",
      "+-----------+-----------------+\n",
      "|     loc196|           29.225|\n",
      "|     loc226|           25.306|\n",
      "|     loc463|           23.317|\n",
      "|     loc150|           32.188|\n",
      "|     loc292|           29.159|\n",
      "|     loc311|           24.308|\n",
      "|      loc22|           28.251|\n",
      "|     loc351|           28.194|\n",
      "|     loc370|            29.14|\n",
      "|     loc419|           29.141|\n",
      "|      loc31|           25.196|\n",
      "|     loc305|           27.314|\n",
      "|      loc82|           27.355|\n",
      "|      loc90|           23.216|\n",
      "|     loc118|           24.219|\n",
      "|     loc195|            27.25|\n",
      "|     loc208|           26.206|\n",
      "|      loc39|           25.199|\n",
      "|      loc75|           23.209|\n",
      "|     loc228|           27.295|\n",
      "+-----------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy(\"location_id\").agg({'temp_celcius':'mean'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n",
      "|location_id|max(temp_celcius)|\n",
      "+-----------+-----------------+\n",
      "|     loc196|               36|\n",
      "|     loc226|               32|\n",
      "|     loc463|               30|\n",
      "|     loc150|               39|\n",
      "|     loc292|               36|\n",
      "|     loc311|               31|\n",
      "|      loc22|               35|\n",
      "|     loc351|               35|\n",
      "|     loc370|               36|\n",
      "|     loc419|               36|\n",
      "|     loc305|               34|\n",
      "|      loc31|               32|\n",
      "|     loc118|               31|\n",
      "|     loc195|               34|\n",
      "|     loc208|               33|\n",
      "|      loc82|               34|\n",
      "|      loc90|               30|\n",
      "|     loc228|               34|\n",
      "|      loc39|               32|\n",
      "|      loc75|               30|\n",
      "+-----------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy(\"location_id\").agg({'temp_celcius':'max'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing 2nd CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path2 = \"/C:/Users/niles/Downloads/IS_Courses/Spark LinkedIn/Ex_Files_Spark_SQL_DataFrames/Exercise Files/Data/utilization.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read.format(\"csv\").option(\"header\",\"false\").option(\"inferSchema\",\"true\").load(data_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+----+----+---+\n",
      "|                _c0|_c1| _c2| _c3|_c4|\n",
      "+-------------------+---+----+----+---+\n",
      "|03/05/2019 08:06:14|100|0.57|0.51| 47|\n",
      "|03/05/2019 08:11:14|100|0.47|0.62| 43|\n",
      "|03/05/2019 08:16:14|100|0.56|0.57| 62|\n",
      "|03/05/2019 08:21:14|100|0.57|0.56| 50|\n",
      "|03/05/2019 08:26:14|100|0.35|0.46| 43|\n",
      "|03/05/2019 08:31:14|100|0.41|0.58| 48|\n",
      "|03/05/2019 08:36:14|100|0.57|0.35| 58|\n",
      "|03/05/2019 08:41:14|100|0.41| 0.4| 58|\n",
      "|03/05/2019 08:46:14|100|0.53|0.35| 62|\n",
      "|03/05/2019 08:51:14|100|0.51| 0.6| 45|\n",
      "|03/05/2019 08:56:14|100|0.32|0.37| 47|\n",
      "|03/05/2019 09:01:14|100|0.62|0.59| 60|\n",
      "|03/05/2019 09:06:14|100|0.66|0.72| 57|\n",
      "|03/05/2019 09:11:14|100|0.54|0.54| 44|\n",
      "|03/05/2019 09:16:14|100|0.29| 0.4| 47|\n",
      "|03/05/2019 09:21:14|100|0.43|0.68| 66|\n",
      "|03/05/2019 09:26:14|100|0.49|0.66| 65|\n",
      "|03/05/2019 09:31:14|100|0.64|0.55| 66|\n",
      "|03/05/2019 09:36:14|100|0.42| 0.6| 42|\n",
      "|03/05/2019 09:41:14|100|0.55|0.59| 63|\n",
      "+-------------------+---+----+----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming the Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.withColumnRenamed(\"_c0\", \"event_datetime\") \\\n",
    "        .withColumnRenamed (\"_c1\", \"server_id\")       \\\n",
    "        .withColumnRenamed(\"_c2\", \"cpu_utilization\")  \\\n",
    "        .withColumnRenamed(\"_c3\", \"free_memory\")      \\\n",
    "        .withColumnRenamed(\"_c4\", \"session_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(event_datetime='03/05/2019 08:06:14', server_id=100, cpu_utilization=0.57, free_memory=0.51, session_count=47),\n",
       " Row(event_datetime='03/05/2019 08:11:14', server_id=100, cpu_utilization=0.47, free_memory=0.62, session_count=43),\n",
       " Row(event_datetime='03/05/2019 08:16:14', server_id=100, cpu_utilization=0.56, free_memory=0.57, session_count=62),\n",
       " Row(event_datetime='03/05/2019 08:21:14', server_id=100, cpu_utilization=0.57, free_memory=0.56, session_count=50),\n",
       " Row(event_datetime='03/05/2019 08:26:14', server_id=100, cpu_utilization=0.35, free_memory=0.46, session_count=43),\n",
       " Row(event_datetime='03/05/2019 08:31:14', server_id=100, cpu_utilization=0.41, free_memory=0.58, session_count=48),\n",
       " Row(event_datetime='03/05/2019 08:36:14', server_id=100, cpu_utilization=0.57, free_memory=0.35, session_count=58),\n",
       " Row(event_datetime='03/05/2019 08:41:14', server_id=100, cpu_utilization=0.41, free_memory=0.4, session_count=58),\n",
       " Row(event_datetime='03/05/2019 08:46:14', server_id=100, cpu_utilization=0.53, free_memory=0.35, session_count=62),\n",
       " Row(event_datetime='03/05/2019 08:51:14', server_id=100, cpu_utilization=0.51, free_memory=0.6, session_count=45)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data_path3 = \"/C:/Users/niles/Downloads/IS_Courses/Spark LinkedIn/Ex_Files_Spark_SQL_DataFrames/Exercise Files/Data/Target_JSON\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "json_df1_path = data_path3 + \"/location_temp.json\"\n",
    "df1.write.json(json_df1_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "json_df2_path = data_path3 + \"/utilization.json\"\n",
    "df2.write.json(json_df2_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WORKING WITH JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path4 = \"/C:/Users/niles/Downloads/IS_Courses/Spark LinkedIn/Ex_Files_Spark_SQL_DataFrames/Exercise Files/Data/utilization.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json = spark.read.format(\"json\").load(data_path4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(cpu_utilization=0.57, event_datetime='03/05/2019 08:06:14', free_memory=0.51, server_id=100, session_count=47),\n",
       " Row(cpu_utilization=0.47, event_datetime='03/05/2019 08:11:14', free_memory=0.62, server_id=100, session_count=43),\n",
       " Row(cpu_utilization=0.56, event_datetime='03/05/2019 08:16:14', free_memory=0.57, server_id=100, session_count=62),\n",
       " Row(cpu_utilization=0.57, event_datetime='03/05/2019 08:21:14', free_memory=0.56, server_id=100, session_count=50),\n",
       " Row(cpu_utilization=0.35, event_datetime='03/05/2019 08:26:14', free_memory=0.46, server_id=100, session_count=43),\n",
       " Row(cpu_utilization=0.41, event_datetime='03/05/2019 08:31:14', free_memory=0.58, server_id=100, session_count=48),\n",
       " Row(cpu_utilization=0.57, event_datetime='03/05/2019 08:36:14', free_memory=0.35, server_id=100, session_count=58),\n",
       " Row(cpu_utilization=0.41, event_datetime='03/05/2019 08:41:14', free_memory=0.4, server_id=100, session_count=58),\n",
       " Row(cpu_utilization=0.53, event_datetime='03/05/2019 08:46:14', free_memory=0.35, server_id=100, session_count=62),\n",
       " Row(cpu_utilization=0.51, event_datetime='03/05/2019 08:51:14', free_memory=0.6, server_id=100, session_count=45)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_json.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|           0.57|03/05/2019 08:06:14|       0.51|      100|           47|\n",
      "|           0.47|03/05/2019 08:11:14|       0.62|      100|           43|\n",
      "|           0.56|03/05/2019 08:16:14|       0.57|      100|           62|\n",
      "|           0.57|03/05/2019 08:21:14|       0.56|      100|           50|\n",
      "|           0.35|03/05/2019 08:26:14|       0.46|      100|           43|\n",
      "|           0.41|03/05/2019 08:31:14|       0.58|      100|           48|\n",
      "|           0.57|03/05/2019 08:36:14|       0.35|      100|           58|\n",
      "|           0.41|03/05/2019 08:41:14|        0.4|      100|           58|\n",
      "|           0.53|03/05/2019 08:46:14|       0.35|      100|           62|\n",
      "|           0.51|03/05/2019 08:51:14|        0.6|      100|           45|\n",
      "|           0.32|03/05/2019 08:56:14|       0.37|      100|           47|\n",
      "|           0.62|03/05/2019 09:01:14|       0.59|      100|           60|\n",
      "|           0.66|03/05/2019 09:06:14|       0.72|      100|           57|\n",
      "|           0.54|03/05/2019 09:11:14|       0.54|      100|           44|\n",
      "|           0.29|03/05/2019 09:16:14|        0.4|      100|           47|\n",
      "|           0.43|03/05/2019 09:21:14|       0.68|      100|           66|\n",
      "|           0.49|03/05/2019 09:26:14|       0.66|      100|           65|\n",
      "|           0.64|03/05/2019 09:31:14|       0.55|      100|           66|\n",
      "|           0.42|03/05/2019 09:36:14|        0.6|      100|           42|\n",
      "|           0.55|03/05/2019 09:41:14|       0.59|      100|           63|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_json.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_json.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cpu_utilization',\n",
       " 'event_datetime',\n",
       " 'free_memory',\n",
       " 'server_id',\n",
       " 'session_count']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_json.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAMPLING with DataFrame API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_sample = df2.sample(False,fraction=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50007"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2_sample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_s1 = df1.sample(fraction = 0.1, withReplacement = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50066"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_s1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|location_id| avg(temp_celcius)|\n",
      "+-----------+------------------+\n",
      "|     loc196| 29.14423076923077|\n",
      "|     loc226|25.877551020408163|\n",
      "|     loc463|23.306306306306308|\n",
      "|     loc150| 32.31683168316832|\n",
      "|     loc292| 29.46153846153846|\n",
      "|     loc311| 23.98095238095238|\n",
      "|      loc22| 28.58139534883721|\n",
      "|     loc351|28.272727272727273|\n",
      "|     loc370|28.990384615384617|\n",
      "|     loc419|28.778846153846153|\n",
      "|      loc31|24.789473684210527|\n",
      "|     loc305|27.511627906976745|\n",
      "|      loc82|27.236363636363638|\n",
      "|      loc90|22.820224719101123|\n",
      "|     loc118| 24.09278350515464|\n",
      "|     loc195| 27.26086956521739|\n",
      "|     loc208|          26.84375|\n",
      "|      loc39|25.009615384615383|\n",
      "|      loc75| 23.28421052631579|\n",
      "|     loc228|26.821428571428573|\n",
      "+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1_s1.groupBy(\"location_id\").agg({'temp_celcius':'mean'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|location_id| avg(temp_celcius)|\n",
      "+-----------+------------------+\n",
      "|       loc0|29.114285714285714|\n",
      "|       loc1|28.215686274509803|\n",
      "|      loc10|24.939393939393938|\n",
      "|     loc100|27.580952380952382|\n",
      "|     loc101|25.233766233766232|\n",
      "|     loc102|29.962616822429908|\n",
      "|     loc103|25.306122448979593|\n",
      "|     loc104|26.115044247787612|\n",
      "|     loc105|26.510204081632654|\n",
      "|     loc106| 27.46938775510204|\n",
      "+-----------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1_s1.groupBy(\"location_id\").agg({'temp_celcius':'mean'}).orderBy(\"location_id\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n",
      "|location_id|avg(temp_celcius)|\n",
      "+-----------+-----------------+\n",
      "|       loc0|           29.176|\n",
      "|       loc1|           28.246|\n",
      "|      loc10|           25.337|\n",
      "|     loc100|           27.297|\n",
      "|     loc101|           25.317|\n",
      "|     loc102|           30.327|\n",
      "|     loc103|           25.341|\n",
      "|     loc104|           26.204|\n",
      "|     loc105|           26.217|\n",
      "|     loc106|           27.201|\n",
      "+-----------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy(\"location_id\").agg({'temp_celcius':'mean'}).orderBy(\"location_id\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df1.write.csv('df1_writ.scv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df1.write.json('df1_writ.scv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPARK SQL: FILTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|           0.57|03/05/2019 08:06:14|       0.51|      100|           47|\n",
      "|           0.47|03/05/2019 08:11:14|       0.62|      100|           43|\n",
      "|           0.56|03/05/2019 08:16:14|       0.57|      100|           62|\n",
      "|           0.57|03/05/2019 08:21:14|       0.56|      100|           50|\n",
      "|           0.35|03/05/2019 08:26:14|       0.46|      100|           43|\n",
      "|           0.41|03/05/2019 08:31:14|       0.58|      100|           48|\n",
      "|           0.57|03/05/2019 08:36:14|       0.35|      100|           58|\n",
      "|           0.41|03/05/2019 08:41:14|        0.4|      100|           58|\n",
      "|           0.53|03/05/2019 08:46:14|       0.35|      100|           62|\n",
      "|           0.51|03/05/2019 08:51:14|        0.6|      100|           45|\n",
      "|           0.32|03/05/2019 08:56:14|       0.37|      100|           47|\n",
      "|           0.62|03/05/2019 09:01:14|       0.59|      100|           60|\n",
      "|           0.66|03/05/2019 09:06:14|       0.72|      100|           57|\n",
      "|           0.54|03/05/2019 09:11:14|       0.54|      100|           44|\n",
      "|           0.29|03/05/2019 09:16:14|        0.4|      100|           47|\n",
      "|           0.43|03/05/2019 09:21:14|       0.68|      100|           66|\n",
      "|           0.49|03/05/2019 09:26:14|       0.66|      100|           65|\n",
      "|           0.64|03/05/2019 09:31:14|       0.55|      100|           66|\n",
      "|           0.42|03/05/2019 09:36:14|        0.6|      100|           42|\n",
      "|           0.55|03/05/2019 09:41:14|       0.59|      100|           63|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_json.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a temp view/table of the dataframe to use Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json.createOrReplaceTempView(\"utilization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sql = spark.sql(\"SELECT * FROM utilization LIMIT 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|           0.57|03/05/2019 08:06:14|       0.51|      100|           47|\n",
      "|           0.47|03/05/2019 08:11:14|       0.62|      100|           43|\n",
      "|           0.56|03/05/2019 08:16:14|       0.57|      100|           62|\n",
      "|           0.57|03/05/2019 08:21:14|       0.56|      100|           50|\n",
      "|           0.35|03/05/2019 08:26:14|       0.46|      100|           43|\n",
      "|           0.41|03/05/2019 08:31:14|       0.58|      100|           48|\n",
      "|           0.57|03/05/2019 08:36:14|       0.35|      100|           58|\n",
      "|           0.41|03/05/2019 08:41:14|        0.4|      100|           58|\n",
      "|           0.53|03/05/2019 08:46:14|       0.35|      100|           62|\n",
      "|           0.51|03/05/2019 08:51:14|        0.6|      100|           45|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sql = spark.sql(\"SELECT server_id, session_count FROM utilization LIMIT 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+\n",
      "|server_id|session_count|\n",
      "+---------+-------------+\n",
      "|      100|           47|\n",
      "|      100|           43|\n",
      "|      100|           62|\n",
      "|      100|           50|\n",
      "|      100|           43|\n",
      "|      100|           48|\n",
      "|      100|           58|\n",
      "|      100|           58|\n",
      "|      100|           62|\n",
      "|      100|           45|\n",
      "+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|s_id| sc|\n",
      "+----+---+\n",
      "| 100| 47|\n",
      "| 100| 43|\n",
      "| 100| 62|\n",
      "| 100| 50|\n",
      "| 100| 43|\n",
      "| 100| 48|\n",
      "| 100| 58|\n",
      "| 100| 58|\n",
      "| 100| 62|\n",
      "| 100| 45|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"SELECT server_id as s_id, session_count as sc FROM utilization LIMIT 10\")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_sql.write.csv(\"server.scv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|           0.66|03/05/2019 08:06:48|       0.31|      120|           54|\n",
      "|           0.58|03/05/2019 08:11:48|       0.38|      120|           64|\n",
      "|           0.55|03/05/2019 08:16:48|       0.61|      120|           54|\n",
      "|            0.7|03/05/2019 08:21:48|       0.35|      120|           80|\n",
      "|            0.6|03/05/2019 08:26:48|       0.39|      120|           71|\n",
      "|           0.53|03/05/2019 08:31:48|       0.35|      120|           49|\n",
      "|           0.73|03/05/2019 08:36:48|       0.42|      120|           73|\n",
      "|           0.41|03/05/2019 08:41:48|        0.6|      120|           72|\n",
      "|           0.62|03/05/2019 08:46:48|       0.57|      120|           57|\n",
      "|           0.67|03/05/2019 08:51:48|       0.44|      120|           78|\n",
      "|           0.67|03/05/2019 08:56:48|       0.38|      120|           73|\n",
      "|           0.39|03/05/2019 09:01:48|       0.47|      120|           58|\n",
      "|            0.5|03/05/2019 09:06:48|       0.29|      120|           78|\n",
      "|           0.38|03/05/2019 09:11:48|       0.38|      120|           56|\n",
      "|           0.53|03/05/2019 09:16:48|       0.38|      120|           47|\n",
      "|           0.74|03/05/2019 09:21:48|       0.25|      120|           69|\n",
      "|           0.53|03/05/2019 09:26:48|       0.57|      120|           73|\n",
      "|           0.54|03/05/2019 09:31:48|       0.64|      120|           65|\n",
      "|            0.7|03/05/2019 09:36:48|       0.52|      120|           55|\n",
      "|           0.54|03/05/2019 09:41:48|       0.27|      120|           74|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"SELECT * FROM utilization WHERE server_id = 120\")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sql.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|           0.32|03/05/2019 11:06:14|       0.35|      100|           71|\n",
      "|           0.35|03/05/2019 12:31:14|       0.42|      100|           71|\n",
      "|           0.29|03/05/2019 13:31:14|       0.36|      100|           71|\n",
      "|           0.49|03/05/2019 16:36:14|       0.66|      100|           71|\n",
      "|           0.32|03/05/2019 16:46:14|       0.58|      100|           72|\n",
      "|           0.44|03/05/2019 18:26:14|       0.52|      100|           72|\n",
      "|           0.45|03/05/2019 21:26:14|       0.47|      100|           71|\n",
      "|           0.34|03/06/2019 01:46:14|       0.55|      100|           71|\n",
      "|           0.61|03/06/2019 05:21:14|       0.35|      100|           71|\n",
      "|           0.45|03/06/2019 07:26:14|       0.66|      100|           72|\n",
      "|           0.57|03/06/2019 09:56:14|       0.38|      100|           71|\n",
      "|           0.29|03/06/2019 10:46:14|       0.63|      100|           72|\n",
      "|           0.42|03/06/2019 11:41:14|       0.72|      100|           71|\n",
      "|            0.5|03/06/2019 13:01:14|       0.64|      100|           71|\n",
      "|            0.4|03/06/2019 15:41:14|       0.57|      100|           72|\n",
      "|           0.48|03/06/2019 21:26:14|       0.63|      100|           71|\n",
      "|           0.64|03/06/2019 22:01:14|        0.4|      100|           72|\n",
      "|           0.64|03/07/2019 01:31:14|       0.69|      100|           72|\n",
      "|           0.56|03/07/2019 02:36:14|       0.53|      100|           71|\n",
      "|           0.46|03/07/2019 05:46:14|       0.46|      100|           71|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"SELECT * FROM utilization WHERE session_count > 70\")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "239659"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sql.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|           0.69|03/31/2019 01:46:49|       0.45|      120|           80|\n",
      "|           0.63|04/03/2019 01:21:49|       0.52|      120|           80|\n",
      "|           0.37|03/31/2019 03:46:49|       0.42|      120|           80|\n",
      "|            0.4|03/30/2019 06:11:49|       0.34|      120|           80|\n",
      "|           0.72|03/31/2019 17:16:49|       0.32|      120|           80|\n",
      "|           0.63|03/29/2019 09:06:49|       0.36|      120|           80|\n",
      "|           0.62|03/31/2019 17:56:49|       0.33|      120|           80|\n",
      "|           0.41|03/29/2019 12:21:49|       0.39|      120|           80|\n",
      "|           0.48|03/31/2019 18:41:49|       0.49|      120|           80|\n",
      "|           0.65|03/30/2019 00:36:49|       0.63|      120|           80|\n",
      "|           0.62|03/31/2019 23:16:49|       0.46|      120|           80|\n",
      "|           0.69|03/29/2019 00:51:49|       0.52|      120|           80|\n",
      "|           0.39|03/31/2019 23:26:49|       0.44|      120|           80|\n",
      "|           0.58|03/30/2019 06:46:49|       0.29|      120|           80|\n",
      "|           0.58|04/01/2019 03:01:49|        0.6|      120|           80|\n",
      "|           0.66|03/30/2019 09:01:49|       0.46|      120|           80|\n",
      "|           0.46|04/01/2019 04:16:49|       0.38|      120|           80|\n",
      "|           0.39|03/30/2019 18:41:49|       0.41|      120|           80|\n",
      "|           0.48|04/01/2019 08:36:49|        0.5|      120|           80|\n",
      "|           0.49|04/01/2019 14:56:49|       0.38|      120|           80|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"SELECT * FROM utilization\\\n",
    "                    WHERE session_count > 70\\\n",
    "                    and server_id = 120\\\n",
    "                    ORDER BY session_count DESC\")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2733"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sql.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPARK SQL: AGGREGATION   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  500000|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_count= spark.sql(\"SELECT count(*) FROM utilization\")\n",
    "df_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  239659|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_count= spark.sql(\"SELECT count(*)\\\n",
    "                    FROM utilization \\\n",
    "                    WHERE session_count > 70\")\n",
    "df_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|server_id|count(1)|\n",
      "+---------+--------+\n",
      "|      101|    9808|\n",
      "|      113|    9418|\n",
      "|      145|    9304|\n",
      "|      103|    8744|\n",
      "|      102|    8586|\n",
      "|      133|    8583|\n",
      "|      108|    8375|\n",
      "|      149|    8288|\n",
      "|      137|    8248|\n",
      "|      148|    8027|\n",
      "|      123|    7918|\n",
      "|      118|    7913|\n",
      "|      112|    7425|\n",
      "|      139|    7383|\n",
      "|      104|    7366|\n",
      "|      121|    7084|\n",
      "|      142|    7084|\n",
      "|      146|    7072|\n",
      "|      126|    6365|\n",
      "|      144|    6220|\n",
      "+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_count= spark.sql(\"SELECT server_id, count(*)\\\n",
    "                    FROM utilization \\\n",
    "                    WHERE session_count > 70 \\\n",
    "                    GROUP BY server_id \\\n",
    "                    ORDER BY count(*) DESC\")\n",
    "df_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------------------+------------------+------------------+\n",
      "|server_id|round(avg(session_count), 2)|max(session_count)|min(session_count)|\n",
      "+---------+----------------------------+------------------+------------------+\n",
      "|      101|                       87.67|               105|                71|\n",
      "|      113|                       86.96|               103|                71|\n",
      "|      145|                       86.98|               103|                71|\n",
      "|      103|                       85.76|               101|                71|\n",
      "|      102|                       85.71|               101|                71|\n",
      "|      133|                       85.47|               100|                71|\n",
      "|      108|                       85.12|               100|                71|\n",
      "|      149|                       84.96|                99|                71|\n",
      "|      137|                       85.01|                99|                71|\n",
      "|      148|                        84.7|                99|                71|\n",
      "|      123|                       84.53|                98|                71|\n",
      "|      118|                       84.66|                98|                71|\n",
      "|      112|                       83.55|                97|                71|\n",
      "|      139|                       83.33|                96|                71|\n",
      "|      104|                       83.35|                96|                71|\n",
      "|      121|                       82.89|                95|                71|\n",
      "|      142|                        82.9|                95|                71|\n",
      "|      146|                       82.95|                95|                71|\n",
      "|      126|                       81.56|                93|                71|\n",
      "|      144|                       81.38|                92|                71|\n",
      "+---------+----------------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_count= spark.sql(\"SELECT server_id, round(mean(session_count), 2), max(session_count), min(session_count)\\\n",
    "                    FROM utilization \\\n",
    "                    WHERE session_count > 70 \\\n",
    "                    GROUP BY server_id \\\n",
    "                    ORDER BY count(*) DESC\")\n",
    "df_count.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPARK SQL: JOINING DATAFRAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|           0.57|03/05/2019 08:06:14|       0.51|      100|           47|\n",
      "|           0.47|03/05/2019 08:11:14|       0.62|      100|           43|\n",
      "|           0.56|03/05/2019 08:16:14|       0.57|      100|           62|\n",
      "|           0.57|03/05/2019 08:21:14|       0.56|      100|           50|\n",
      "|           0.35|03/05/2019 08:26:14|       0.46|      100|           43|\n",
      "|           0.41|03/05/2019 08:31:14|       0.58|      100|           48|\n",
      "|           0.57|03/05/2019 08:36:14|       0.35|      100|           58|\n",
      "|           0.41|03/05/2019 08:41:14|        0.4|      100|           58|\n",
      "|           0.53|03/05/2019 08:46:14|       0.35|      100|           62|\n",
      "|           0.51|03/05/2019 08:51:14|        0.6|      100|           45|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_json.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = \"/C:/Users/niles/Downloads/IS_Courses/Spark LinkedIn/Ex_Files_Spark_SQL_DataFrames/Exercise Files/Data/location_temp.csv\"\n",
    "\n",
    "df_server = spark.read.format(\"csv\").option(\"header\",\"true\").load(dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+------------+\n",
      "|         event_date|location_id|temp_celcius|\n",
      "+-------------------+-----------+------------+\n",
      "|03/04/2019 19:48:06|       loc0|          29|\n",
      "|03/04/2019 19:53:06|       loc0|          27|\n",
      "|03/04/2019 19:58:06|       loc0|          28|\n",
      "|03/04/2019 20:03:06|       loc0|          30|\n",
      "|03/04/2019 20:08:06|       loc0|          27|\n",
      "|03/04/2019 20:13:06|       loc0|          27|\n",
      "|03/04/2019 20:18:06|       loc0|          27|\n",
      "|03/04/2019 20:23:06|       loc0|          29|\n",
      "|03/04/2019 20:28:06|       loc0|          32|\n",
      "|03/04/2019 20:33:06|       loc0|          35|\n",
      "+-------------------+-----------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_server.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a temp view/table of the dataframe to use Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_server.createOrReplaceTempView(\"server_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_join = spark.sql(\"SELECT * \\\n",
    "                   FROM utilization u \\\n",
    "                   INNER JOIN server_name s \\\n",
    "                   ON day(u.even_datetime) = day(s.event_date)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark import SparkContext, SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OR\n",
    "\n",
    "\n",
    "# from pyspark import SparkConf, SparkContext, SQLContext\n",
    "\n",
    "# conf = SparkConf().setAppName(\"test\").setMaster(\"local\")\n",
    "# sc = SparkContext(conf=conf)\n",
    "# sqlContext = SQLContext(sc)\n",
    "# print(sc)\n",
    "# print(sqlContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dup = sc.parallelize([Row(server_name='101 Server', cpu_utilization=85, session_count=80), \\\n",
    "                             Row(server_name='101 Server', cpu_utilization=80, session_count=90),\n",
    "                             Row(server_name='102 Server', cpu_utilization=85, session_count=80),\n",
    "                             Row(server_name='102 Server', cpu_utilization=85, session_count=80)]).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+-------------+\n",
      "|cpu_utilization|server_name|session_count|\n",
      "+---------------+-----------+-------------+\n",
      "|             85| 101 Server|           80|\n",
      "|             80| 101 Server|           90|\n",
      "|             85| 102 Server|           80|\n",
      "|             85| 102 Server|           80|\n",
      "+---------------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dup.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+-------------+\n",
      "|cpu_utilization|server_name|session_count|\n",
      "+---------------+-----------+-------------+\n",
      "|             85| 101 Server|           80|\n",
      "|             80| 101 Server|           90|\n",
      "|             85| 102 Server|           80|\n",
      "+---------------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dup.drop_duplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+-------------+\n",
      "|cpu_utilization|server_name|session_count|\n",
      "+---------------+-----------+-------------+\n",
      "|             85| 102 Server|           80|\n",
      "|             85| 101 Server|           80|\n",
      "+---------------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dup.drop_duplicates(['server_name']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WORKING WITH NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sc.parallelize([Row(server_name='101 Server', cpu_utilization=85, session_count=80), \\\n",
    "                             Row(server_name='101 Server', cpu_utilization=80, session_count=90),\n",
    "                             Row(server_name='102 Server', cpu_utilization=85, session_count=40),\n",
    "                             Row(server_name='103 Server', cpu_utilization=70, session_count=80),\n",
    "                             Row(server_name='104 Server', cpu_utilization=60, session_count=80)]).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+-------------+\n",
      "|cpu_utilization|server_name|session_count|\n",
      "+---------------+-----------+-------------+\n",
      "|             85| 101 Server|           80|\n",
      "|             80| 101 Server|           90|\n",
      "|             85| 102 Server|           40|\n",
      "|             70| 103 Server|           80|\n",
      "|             60| 104 Server|           80|\n",
      "+---------------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a new Column with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_na = df.withColumn('na_col', lit(None).cast(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+-------------+------+\n",
      "|cpu_utilization|server_name|session_count|na_col|\n",
      "+---------------+-----------+-------------+------+\n",
      "|             85| 101 Server|           80|  null|\n",
      "|             80| 101 Server|           90|  null|\n",
      "|             85| 102 Server|           40|  null|\n",
      "|             70| 103 Server|           80|  null|\n",
      "|             60| 104 Server|           80|  null|\n",
      "+---------------+-----------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_na.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+-------------+------+\n",
      "|cpu_utilization|server_name|session_count|na_col|\n",
      "+---------------+-----------+-------------+------+\n",
      "|             85| 101 Server|           80|     A|\n",
      "|             80| 101 Server|           90|     A|\n",
      "|             85| 102 Server|           40|     A|\n",
      "|             70| 103 Server|           80|     A|\n",
      "|             60| 104 Server|           80|     A|\n",
      "+---------------+-----------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_na.fillna('A').show()  #filling null values with A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df_na.fillna('A').union(df_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+-------------+------+\n",
      "|cpu_utilization|server_name|session_count|na_col|\n",
      "+---------------+-----------+-------------+------+\n",
      "|             85| 101 Server|           80|     A|\n",
      "|             80| 101 Server|           90|     A|\n",
      "|             85| 102 Server|           40|     A|\n",
      "|             70| 103 Server|           80|     A|\n",
      "|             60| 104 Server|           80|     A|\n",
      "|             85| 101 Server|           80|  null|\n",
      "|             80| 101 Server|           90|  null|\n",
      "|             85| 102 Server|           40|  null|\n",
      "|             70| 103 Server|           80|  null|\n",
      "|             60| 104 Server|           80|  null|\n",
      "+---------------+-----------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+-------------+------+\n",
      "|cpu_utilization|server_name|session_count|na_col|\n",
      "+---------------+-----------+-------------+------+\n",
      "|             85| 101 Server|           80|     A|\n",
      "|             80| 101 Server|           90|     A|\n",
      "|             85| 102 Server|           40|     A|\n",
      "|             70| 103 Server|           80|     A|\n",
      "|             60| 104 Server|           80|     A|\n",
      "+---------------+-----------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+-------------+------+\n",
      "|cpu_utilization|server_name|session_count|na_col|\n",
      "+---------------+-----------+-------------+------+\n",
      "|             85| 101 Server|           80|     A|\n",
      "|             80| 101 Server|           90|     A|\n",
      "|             85| 102 Server|           40|     A|\n",
      "|             70| 103 Server|           80|     A|\n",
      "|             60| 104 Server|           80|     A|\n",
      "+---------------+-----------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.dropna().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+-------------+------+\n",
      "|cpu_utilization|server_name|session_count|na_col|\n",
      "+---------------+-----------+-------------+------+\n",
      "|             85| 101 Server|           80|     A|\n",
      "|             80| 101 Server|           90|     A|\n",
      "|             85| 102 Server|           40|     A|\n",
      "|             70| 103 Server|           80|     A|\n",
      "|             60| 104 Server|           80|     A|\n",
      "|             85| 101 Server|           80|  null|\n",
      "|             80| 101 Server|           90|  null|\n",
      "|             85| 102 Server|           40|  null|\n",
      "|             70| 103 Server|           80|  null|\n",
      "|             60| 104 Server|           80|  null|\n",
      "+---------------+-----------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a temp view/table of the dataframe to use Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.createOrReplaceTempView(\"na_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+-------------+------+\n",
      "|cpu_utilization|server_name|session_count|na_col|\n",
      "+---------------+-----------+-------------+------+\n",
      "|             85| 101 Server|           80|     A|\n",
      "|             80| 101 Server|           90|     A|\n",
      "|             85| 102 Server|           40|     A|\n",
      "|             70| 103 Server|           80|     A|\n",
      "|             60| 104 Server|           80|     A|\n",
      "|             85| 101 Server|           80|  null|\n",
      "|             80| 101 Server|           90|  null|\n",
      "|             85| 102 Server|           40|  null|\n",
      "|             70| 103 Server|           80|  null|\n",
      "|             60| 104 Server|           80|  null|\n",
      "+---------------+-----------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM na_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+-------------+------+\n",
      "|cpu_utilization|server_name|session_count|na_col|\n",
      "+---------------+-----------+-------------+------+\n",
      "|             85| 101 Server|           80|  null|\n",
      "|             80| 101 Server|           90|  null|\n",
      "|             85| 102 Server|           40|  null|\n",
      "|             70| 103 Server|           80|  null|\n",
      "|             60| 104 Server|           80|  null|\n",
      "+---------------+-----------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM na_table WHERE na_col IS NULL\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+-------------+------+\n",
      "|cpu_utilization|server_name|session_count|na_col|\n",
      "+---------------+-----------+-------------+------+\n",
      "|             85| 101 Server|           80|     A|\n",
      "|             80| 101 Server|           90|     A|\n",
      "|             85| 102 Server|           40|     A|\n",
      "|             70| 103 Server|           80|     A|\n",
      "|             60| 104 Server|           80|     A|\n",
      "+---------------+-----------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM na_table WHERE na_col IS NOT NULL\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
